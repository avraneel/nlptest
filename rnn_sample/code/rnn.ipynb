{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # when loading file paths\n",
    "import pandas as pd  # for lookup in annotation file\n",
    "import spacy  # for tokenizer\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence  # pad batch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We want to convert text -> numerical values\n",
    "# 1. We need a Vocabulary mapping each word to a index\n",
    "# 2. We need to setup a Pytorch dataset to load the data\n",
    "# 3. Setup padding of every batch (all examples should be\n",
    "#    of same seq_len and setup dataloader)\n",
    "# Note that loading the image is very easy compared to the text!\n",
    "\n",
    "# Download with: python -m spacy download en_core_web_sm\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "        #print(tokenized_text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]\n",
    "\n",
    "\n",
    "class NewDelhiDataset(Dataset):\n",
    "    def __init__(self, file, freq_threshold=5):\n",
    "        self.df = pd.read_csv(file)\n",
    "\n",
    "        self.reviews = self.df[\"review_full\"]\n",
    "        self.ratings = self.df[\"rating_review\"]\n",
    "        \n",
    "        # Initialize vocabulary and build vocab\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.reviews.tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rating = self.ratings[index]\n",
    "        review = self.reviews[index]\n",
    "        \n",
    "        numericalized_rating = [0,0,0,0,0]\n",
    "        numericalized_rating[rating-1] = 1\n",
    "        \n",
    "        numericalized_review = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_review += self.vocab.numericalize(review)\n",
    "        numericalized_review.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        return numericalized_rating, torch.tensor(numericalized_review)\n",
    "\n",
    "\n",
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        ratings = [item[0] for item in batch]\n",
    "        # ratings_encoded = [0,0,0,0,0]\n",
    "        # ratings_encoded[ratings-1] = 1\n",
    "        \n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx)\n",
    "\n",
    "        return ratings, targets\n",
    "\n",
    "\n",
    "def get_loader(\n",
    "    annotation_file,\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "):\n",
    "    dataset = NewDelhiDataset(annotation_file)\n",
    "\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
    "    )\n",
    "\n",
    "    return loader, dataset\n",
    "\n",
    "loader, dataset = get_loader(\"../data/New_Delhi_reviews_rnn.csv\")    \n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1]]\n",
      "torch.Size([32, 511])\n"
     ]
    }
   ],
   "source": [
    "for i, (ratings, reviews) in enumerate(loader):\n",
    "    print(ratings)\n",
    "    print(reviews.shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_classes = 5\n",
    "num_epochs = 2\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "input_size = 1\n",
    "hidden_size = 1\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # -> x needs to be: (batch_size, seq, input_size)\n",
    "        \n",
    "        # or:\n",
    "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        h0 = torch.zeros(num_layers, x.size(0), hidden_size).to(device) \n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        # x: (32, L, 1), h0: (32, L, 1)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.rnn(x, h0)  \n",
    "        # or:\n",
    "        #out, _ = self.lstm(x, (h0,c0))  \n",
    "        \n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # out: (n, 28, 128)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, :, :]\n",
    "        # out: (n, 1)\n",
    "         \n",
    "        out = self.fc(out)\n",
    "        # out: (n, 10)\n",
    "        return out\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/4612], Loss: -5.8727\n",
      "Epoch [1/5], Step [200/4612], Loss: -6.0727\n",
      "Epoch [1/5], Step [300/4612], Loss: -6.2727\n",
      "Epoch [1/5], Step [400/4612], Loss: -6.4727\n",
      "Epoch [1/5], Step [500/4612], Loss: -6.6727\n",
      "Epoch [1/5], Step [600/4612], Loss: -6.8727\n",
      "Epoch [1/5], Step [700/4612], Loss: -7.0727\n",
      "Epoch [1/5], Step [800/4612], Loss: -7.2726\n",
      "Epoch [1/5], Step [900/4612], Loss: -7.4726\n",
      "Epoch [1/5], Step [1000/4612], Loss: -7.6726\n",
      "Epoch [1/5], Step [1100/4612], Loss: -7.8726\n",
      "Epoch [1/5], Step [1200/4612], Loss: -8.0726\n",
      "Epoch [1/5], Step [1300/4612], Loss: -8.2726\n",
      "Epoch [1/5], Step [1400/4612], Loss: -8.4726\n",
      "Epoch [1/5], Step [1500/4612], Loss: -8.6726\n",
      "Epoch [1/5], Step [1600/4612], Loss: -8.8725\n",
      "Epoch [1/5], Step [1700/4612], Loss: -9.0725\n",
      "Epoch [1/5], Step [1800/4612], Loss: -9.2725\n",
      "Epoch [1/5], Step [1900/4612], Loss: -9.4725\n",
      "Epoch [1/5], Step [2000/4612], Loss: -9.6725\n",
      "Epoch [1/5], Step [2100/4612], Loss: -9.8725\n",
      "Epoch [1/5], Step [2200/4612], Loss: -10.0725\n",
      "Epoch [1/5], Step [2300/4612], Loss: -10.2724\n",
      "Epoch [1/5], Step [2400/4612], Loss: -10.4724\n",
      "Epoch [1/5], Step [2500/4612], Loss: -10.6724\n",
      "Epoch [1/5], Step [2600/4612], Loss: -10.8724\n",
      "Epoch [1/5], Step [2700/4612], Loss: -11.0724\n",
      "Epoch [1/5], Step [2800/4612], Loss: -11.2724\n",
      "Epoch [1/5], Step [2900/4612], Loss: -11.4723\n",
      "Epoch [1/5], Step [3000/4612], Loss: -11.6723\n",
      "Epoch [1/5], Step [3100/4612], Loss: -11.8723\n",
      "Epoch [1/5], Step [3200/4612], Loss: -12.0723\n",
      "Epoch [1/5], Step [3300/4612], Loss: -12.2723\n",
      "Epoch [1/5], Step [3400/4612], Loss: -12.4723\n",
      "Epoch [1/5], Step [3500/4612], Loss: -12.6723\n",
      "Epoch [1/5], Step [3600/4612], Loss: -12.8722\n",
      "Epoch [1/5], Step [3700/4612], Loss: -13.0722\n",
      "Epoch [1/5], Step [3800/4612], Loss: -13.2722\n",
      "Epoch [1/5], Step [3900/4612], Loss: -13.4722\n",
      "Epoch [1/5], Step [4000/4612], Loss: -13.6722\n",
      "Epoch [1/5], Step [4100/4612], Loss: -13.8722\n",
      "Epoch [1/5], Step [4200/4612], Loss: -14.0722\n",
      "Epoch [1/5], Step [4300/4612], Loss: -14.2721\n",
      "Epoch [1/5], Step [4400/4612], Loss: -14.4721\n",
      "Epoch [1/5], Step [4500/4612], Loss: -14.6721\n",
      "Epoch [1/5], Step [4600/4612], Loss: -14.8721\n",
      "Epoch [2/5], Step [100/4612], Loss: -15.0961\n",
      "Epoch [2/5], Step [200/4612], Loss: -15.2961\n",
      "Epoch [2/5], Step [300/4612], Loss: -15.4961\n",
      "Epoch [2/5], Step [400/4612], Loss: -15.6962\n",
      "Epoch [2/5], Step [500/4612], Loss: -15.8962\n",
      "Epoch [2/5], Step [600/4612], Loss: -16.0962\n",
      "Epoch [2/5], Step [700/4612], Loss: -16.2962\n",
      "Epoch [2/5], Step [800/4612], Loss: -16.4963\n",
      "Epoch [2/5], Step [900/4612], Loss: -16.6963\n",
      "Epoch [2/5], Step [1000/4612], Loss: -16.8964\n",
      "Epoch [2/5], Step [1100/4612], Loss: -17.0965\n",
      "Epoch [2/5], Step [1200/4612], Loss: -17.2965\n",
      "Epoch [2/5], Step [1300/4612], Loss: -17.4966\n",
      "Epoch [2/5], Step [1400/4612], Loss: -17.6967\n",
      "Epoch [2/5], Step [1500/4612], Loss: -17.8968\n",
      "Epoch [2/5], Step [1600/4612], Loss: -18.0969\n",
      "Epoch [2/5], Step [1700/4612], Loss: -18.2970\n",
      "Epoch [2/5], Step [1800/4612], Loss: -18.4970\n",
      "Epoch [2/5], Step [1900/4612], Loss: -18.6971\n",
      "Epoch [2/5], Step [2000/4612], Loss: -18.8972\n",
      "Epoch [2/5], Step [2100/4612], Loss: -19.0973\n",
      "Epoch [2/5], Step [2200/4612], Loss: -19.2974\n",
      "Epoch [2/5], Step [2300/4612], Loss: -19.4974\n",
      "Epoch [2/5], Step [2400/4612], Loss: -19.6975\n",
      "Epoch [2/5], Step [2500/4612], Loss: -19.8976\n",
      "Epoch [2/5], Step [2600/4612], Loss: -20.0977\n",
      "Epoch [2/5], Step [2700/4612], Loss: -20.2978\n",
      "Epoch [2/5], Step [2800/4612], Loss: -20.4978\n",
      "Epoch [2/5], Step [2900/4612], Loss: -20.6979\n",
      "Epoch [2/5], Step [3000/4612], Loss: -20.8980\n",
      "Epoch [2/5], Step [3100/4612], Loss: -21.0981\n",
      "Epoch [2/5], Step [3200/4612], Loss: -21.2982\n",
      "Epoch [2/5], Step [3300/4612], Loss: -21.4982\n",
      "Epoch [2/5], Step [3400/4612], Loss: -21.6983\n",
      "Epoch [2/5], Step [3500/4612], Loss: -21.8984\n",
      "Epoch [2/5], Step [3600/4612], Loss: -22.0985\n",
      "Epoch [2/5], Step [3700/4612], Loss: -22.2986\n",
      "Epoch [2/5], Step [3800/4612], Loss: -22.4986\n",
      "Epoch [2/5], Step [3900/4612], Loss: -22.6987\n",
      "Epoch [2/5], Step [4000/4612], Loss: -22.8988\n",
      "Epoch [2/5], Step [4100/4612], Loss: -23.0989\n",
      "Epoch [2/5], Step [4200/4612], Loss: -23.2990\n",
      "Epoch [2/5], Step [4300/4612], Loss: -23.4991\n",
      "Epoch [2/5], Step [4400/4612], Loss: -23.6991\n",
      "Epoch [2/5], Step [4500/4612], Loss: -23.8992\n",
      "Epoch [2/5], Step [4600/4612], Loss: -24.0993\n",
      "Epoch [3/5], Step [100/4612], Loss: -24.3234\n",
      "Epoch [3/5], Step [200/4612], Loss: -24.5235\n",
      "Epoch [3/5], Step [300/4612], Loss: -24.7235\n",
      "Epoch [3/5], Step [400/4612], Loss: -24.9236\n",
      "Epoch [3/5], Step [500/4612], Loss: -25.1237\n",
      "Epoch [3/5], Step [600/4612], Loss: -25.3238\n",
      "Epoch [3/5], Step [700/4612], Loss: -25.5239\n",
      "Epoch [3/5], Step [800/4612], Loss: -25.7240\n",
      "Epoch [3/5], Step [900/4612], Loss: -25.9240\n",
      "Epoch [3/5], Step [1000/4612], Loss: -26.1241\n",
      "Epoch [3/5], Step [1100/4612], Loss: -26.3242\n",
      "Epoch [3/5], Step [1200/4612], Loss: -26.5243\n",
      "Epoch [3/5], Step [1300/4612], Loss: -26.7244\n",
      "Epoch [3/5], Step [1400/4612], Loss: -26.9244\n",
      "Epoch [3/5], Step [1500/4612], Loss: -27.1245\n",
      "Epoch [3/5], Step [1600/4612], Loss: -27.3246\n",
      "Epoch [3/5], Step [1700/4612], Loss: -27.5247\n",
      "Epoch [3/5], Step [1800/4612], Loss: -27.7248\n",
      "Epoch [3/5], Step [1900/4612], Loss: -27.9248\n",
      "Epoch [3/5], Step [2000/4612], Loss: -28.1249\n",
      "Epoch [3/5], Step [2100/4612], Loss: -28.3250\n",
      "Epoch [3/5], Step [2200/4612], Loss: -28.5251\n",
      "Epoch [3/5], Step [2300/4612], Loss: -28.7252\n",
      "Epoch [3/5], Step [2400/4612], Loss: -28.9253\n",
      "Epoch [3/5], Step [2500/4612], Loss: -29.1253\n",
      "Epoch [3/5], Step [2600/4612], Loss: -29.3254\n",
      "Epoch [3/5], Step [2700/4612], Loss: -29.5255\n",
      "Epoch [3/5], Step [2800/4612], Loss: -29.7256\n",
      "Epoch [3/5], Step [2900/4612], Loss: -29.9256\n",
      "Epoch [3/5], Step [3000/4612], Loss: -30.1257\n",
      "Epoch [3/5], Step [3100/4612], Loss: -30.3258\n",
      "Epoch [3/5], Step [3200/4612], Loss: -30.5259\n",
      "Epoch [3/5], Step [3300/4612], Loss: -30.7259\n",
      "Epoch [3/5], Step [3400/4612], Loss: -30.9260\n",
      "Epoch [3/5], Step [3500/4612], Loss: -31.1260\n",
      "Epoch [3/5], Step [3600/4612], Loss: -31.3261\n",
      "Epoch [3/5], Step [3700/4612], Loss: -31.5261\n",
      "Epoch [3/5], Step [3800/4612], Loss: -31.7261\n",
      "Epoch [3/5], Step [3900/4612], Loss: -31.9261\n",
      "Epoch [3/5], Step [4000/4612], Loss: -32.1261\n",
      "Epoch [3/5], Step [4100/4612], Loss: -32.3261\n",
      "Epoch [3/5], Step [4200/4612], Loss: -32.5261\n",
      "Epoch [3/5], Step [4300/4612], Loss: -32.7260\n",
      "Epoch [3/5], Step [4400/4612], Loss: -32.9260\n",
      "Epoch [3/5], Step [4500/4612], Loss: -33.1259\n",
      "Epoch [3/5], Step [4600/4612], Loss: -33.3258\n",
      "Epoch [4/5], Step [100/4612], Loss: -33.5496\n",
      "Epoch [4/5], Step [200/4612], Loss: -33.7495\n",
      "Epoch [4/5], Step [300/4612], Loss: -33.9494\n",
      "Epoch [4/5], Step [400/4612], Loss: -34.1493\n",
      "Epoch [4/5], Step [500/4612], Loss: -34.3492\n",
      "Epoch [4/5], Step [600/4612], Loss: -34.5491\n",
      "Epoch [4/5], Step [700/4612], Loss: -34.7490\n",
      "Epoch [4/5], Step [800/4612], Loss: -34.9489\n",
      "Epoch [4/5], Step [900/4612], Loss: -35.1488\n",
      "Epoch [4/5], Step [1000/4612], Loss: -35.3486\n",
      "Epoch [4/5], Step [1100/4612], Loss: -35.5485\n",
      "Epoch [4/5], Step [1200/4612], Loss: -35.7484\n",
      "Epoch [4/5], Step [1300/4612], Loss: -35.9483\n",
      "Epoch [4/5], Step [1400/4612], Loss: -36.1482\n",
      "Epoch [4/5], Step [1500/4612], Loss: -36.3481\n",
      "Epoch [4/5], Step [1600/4612], Loss: -36.5480\n",
      "Epoch [4/5], Step [1700/4612], Loss: -36.7479\n",
      "Epoch [4/5], Step [1800/4612], Loss: -36.9478\n",
      "Epoch [4/5], Step [1900/4612], Loss: -37.1477\n",
      "Epoch [4/5], Step [2000/4612], Loss: -37.3476\n",
      "Epoch [4/5], Step [2100/4612], Loss: -37.5475\n",
      "Epoch [4/5], Step [2200/4612], Loss: -37.7473\n",
      "Epoch [4/5], Step [2300/4612], Loss: -37.9472\n",
      "Epoch [4/5], Step [2400/4612], Loss: -38.1471\n",
      "Epoch [4/5], Step [2500/4612], Loss: -38.3470\n",
      "Epoch [4/5], Step [2600/4612], Loss: -38.5469\n",
      "Epoch [4/5], Step [2700/4612], Loss: -38.7468\n",
      "Epoch [4/5], Step [2800/4612], Loss: -38.9467\n",
      "Epoch [4/5], Step [2900/4612], Loss: -39.1466\n",
      "Epoch [4/5], Step [3000/4612], Loss: -39.3465\n",
      "Epoch [4/5], Step [3100/4612], Loss: -39.5464\n",
      "Epoch [4/5], Step [3200/4612], Loss: -39.7462\n",
      "Epoch [4/5], Step [3300/4612], Loss: -39.9461\n",
      "Epoch [4/5], Step [3400/4612], Loss: -40.1460\n",
      "Epoch [4/5], Step [3500/4612], Loss: -40.3459\n",
      "Epoch [4/5], Step [3600/4612], Loss: -40.5458\n",
      "Epoch [4/5], Step [3700/4612], Loss: -40.7457\n",
      "Epoch [4/5], Step [3800/4612], Loss: -40.9456\n",
      "Epoch [4/5], Step [3900/4612], Loss: -41.1455\n",
      "Epoch [4/5], Step [4000/4612], Loss: -41.3453\n",
      "Epoch [4/5], Step [4100/4612], Loss: -41.5452\n",
      "Epoch [4/5], Step [4200/4612], Loss: -41.7451\n",
      "Epoch [4/5], Step [4300/4612], Loss: -41.9450\n",
      "Epoch [4/5], Step [4400/4612], Loss: -42.1449\n",
      "Epoch [4/5], Step [4500/4612], Loss: -42.3448\n",
      "Epoch [4/5], Step [4600/4612], Loss: -42.5447\n",
      "Epoch [5/5], Step [100/4612], Loss: -42.7686\n",
      "Epoch [5/5], Step [200/4612], Loss: -42.9685\n",
      "Epoch [5/5], Step [300/4612], Loss: -43.1684\n",
      "Epoch [5/5], Step [400/4612], Loss: -43.3683\n",
      "Epoch [5/5], Step [500/4612], Loss: -43.5681\n",
      "Epoch [5/5], Step [600/4612], Loss: -43.7680\n",
      "Epoch [5/5], Step [700/4612], Loss: -43.9679\n",
      "Epoch [5/5], Step [800/4612], Loss: -44.1678\n",
      "Epoch [5/5], Step [900/4612], Loss: -44.3677\n",
      "Epoch [5/5], Step [1000/4612], Loss: -44.5676\n",
      "Epoch [5/5], Step [1100/4612], Loss: -44.7675\n",
      "Epoch [5/5], Step [1200/4612], Loss: -44.9674\n",
      "Epoch [5/5], Step [1300/4612], Loss: -45.1673\n",
      "Epoch [5/5], Step [1400/4612], Loss: -45.3672\n",
      "Epoch [5/5], Step [1500/4612], Loss: -45.5670\n",
      "Epoch [5/5], Step [1600/4612], Loss: -45.7669\n",
      "Epoch [5/5], Step [1700/4612], Loss: -45.9668\n",
      "Epoch [5/5], Step [1800/4612], Loss: -46.1667\n",
      "Epoch [5/5], Step [1900/4612], Loss: -46.3666\n",
      "Epoch [5/5], Step [2000/4612], Loss: -46.5665\n",
      "Epoch [5/5], Step [2100/4612], Loss: -46.7664\n",
      "Epoch [5/5], Step [2200/4612], Loss: -46.9663\n",
      "Epoch [5/5], Step [2300/4612], Loss: -47.1661\n",
      "Epoch [5/5], Step [2400/4612], Loss: -47.3660\n",
      "Epoch [5/5], Step [2500/4612], Loss: -47.5659\n",
      "Epoch [5/5], Step [2600/4612], Loss: -47.7658\n",
      "Epoch [5/5], Step [2700/4612], Loss: -47.9657\n",
      "Epoch [5/5], Step [2800/4612], Loss: -48.1656\n",
      "Epoch [5/5], Step [2900/4612], Loss: -48.3655\n",
      "Epoch [5/5], Step [3000/4612], Loss: -48.5654\n",
      "Epoch [5/5], Step [3100/4612], Loss: -48.7653\n",
      "Epoch [5/5], Step [3200/4612], Loss: -48.9652\n",
      "Epoch [5/5], Step [3300/4612], Loss: -49.1650\n",
      "Epoch [5/5], Step [3400/4612], Loss: -49.3649\n",
      "Epoch [5/5], Step [3500/4612], Loss: -49.5648\n",
      "Epoch [5/5], Step [3600/4612], Loss: -49.7647\n",
      "Epoch [5/5], Step [3700/4612], Loss: -49.9646\n",
      "Epoch [5/5], Step [3800/4612], Loss: -50.1645\n",
      "Epoch [5/5], Step [3900/4612], Loss: -50.3644\n",
      "Epoch [5/5], Step [4000/4612], Loss: -50.5643\n",
      "Epoch [5/5], Step [4100/4612], Loss: -50.7642\n",
      "Epoch [5/5], Step [4200/4612], Loss: -50.9641\n",
      "Epoch [5/5], Step [4300/4612], Loss: -51.1640\n",
      "Epoch [5/5], Step [4400/4612], Loss: -51.3639\n",
      "Epoch [5/5], Step [4500/4612], Loss: -51.5638\n",
      "Epoch [5/5], Step [4600/4612], Loss: -51.7636\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "n_total_steps = len(loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (ratings, reviews) in enumerate(loader):  \n",
    "        # ratings_tensor = torch.from_numpy(ratings)\n",
    "        # ratings = ratings.to(device)\n",
    "        ratings_tensor = torch.tensor(ratings)\n",
    "        ratings_tensor = ratings_tensor.to(device)\n",
    "        N = reviews.shape[0]\n",
    "        L = reviews.shape[1]\n",
    "        reviews_tensor = torch.reshape(reviews, (N, L, input_size))\n",
    "        reviews_tensor = reviews_tensor.float()\n",
    "        reviews_tensor = reviews_tensor.to(device)        \n",
    "        \n",
    "        # print(reviews_tensor.dtype)\n",
    "        # Forward pass\n",
    "        outputs = model(reviews_tensor).to(device)\n",
    "        loss = criterion(outputs, ratings_tensor)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
